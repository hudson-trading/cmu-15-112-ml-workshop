{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is just a preamble that sets a bunch of options up.\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Make graphs a little prettier\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (15, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small data set of height in inches and weight in pounds of children\n",
    "height_and_weight = [\n",
    "    [42.8, 40.0],\n",
    "    [63.5, 93.5],\n",
    "    [37.5, 35.5],\n",
    "    [39.5, 30.0],\n",
    "    [45.5, 52.0],\n",
    "    [38.5, 17.0],\n",
    "    [43.0, 38.5],\n",
    "    [22.5,  8.5],\n",
    "    [37.0, 33.0],\n",
    "    [23.5,  9.5],\n",
    "    [33.0, 21.0],\n",
    "    [58.0, 79.0]]\n",
    "\n",
    "# Use pandas to load the data we have into 2 columns - heights and weights.\n",
    "df = pd.DataFrame(height_and_weight, columns=['height', 'weight'])\n",
    "\n",
    "# Scatter plot the data\n",
    "plt.xlabel('Height (in)')\n",
    "plt.ylabel('Weight (lb)')\n",
    "plt.plot(df['height'], df['weight'], '.', markersize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.as_matrix(['height'])\n",
    "Y = np.array(df['weight'])\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Fit the model to our data\n",
    "model = LinearRegression()\n",
    "model.fit(X, Y)\n",
    "\n",
    "# Same scatter plot as before\n",
    "plt.xlabel('Height (in)')\n",
    "plt.ylabel('Weight (lb)')\n",
    "plt.plot(df['height'], df['weight'], '.', markersize=12)\n",
    "\n",
    "# Line showing what our model predicts for heights in the range [20, 65]\n",
    "plt.plot([20, 65], model.predict([[20], [65]]))\n",
    "\n",
    "# But what exactly is this line?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlim(20, 65)\n",
    "\n",
    "# Scatter plot and linear regression line.\n",
    "plt.xlabel('Height (in)')\n",
    "plt.ylabel('Weight (lb)')\n",
    "plt.plot(df['height'], df['weight'], '.', markersize=12)\n",
    "plt.plot([20, 65], model.predict([[20], [65]]))\n",
    "\n",
    "# 1) It passes through the means of the heights and weights\n",
    "plt.scatter([df['height'].mean()], [df['weight'].mean()], s=100)\n",
    "\n",
    "# 2) It minimizes the sum of squares of the errors (the difference of the predicted and actual values):\n",
    "for height, weight in zip(df['height'], df['weight']):\n",
    "    pred_weight = model.predict([[height]])\n",
    "    plt.plot([height, height], [weight, pred_weight], color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make some predictions!\n",
    "\n",
    "print('Height = 50, predicted weight = %.3f' % model.predict([[50]]))\n",
    "print('Height = 22, predicted weight = %.3f' % model.predict([[22]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\"In mathematics, **extrapolation** is the process of estimating, beyond the original observation range, the value of a variable on the basis of its relationship with another variable.\" - https://en.wikipedia.org/wiki/Extrapolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Side note #1: Why do we minimize the square of the errors, and not simply the sum of the errors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# However, take a look at this example.\n",
    "points = [[0, 0], [0, 2], [2, 0], [2, 2]]\n",
    "plt.plot([x for x, y in points], [y for x, y in points] ,\n",
    "         '.', markersize=12)\n",
    "plt.plot([-0.5, 2.5], [1, 1], '-')\n",
    "\n",
    "for x, y in points:\n",
    "    plt.plot([x, x], [y, 1], '--', color='red')\n",
    "\n",
    "plt.xlim(-0.5, 2.5)\n",
    "plt.ylim(-0.5, 2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.html import widgets\n",
    "import math\n",
    "\n",
    "points = [[0, 0], [0, 2], [2, 0], [2, 2]]\n",
    "def graph(i):\n",
    "    plt.plot([x for x, y in points], [y for x, y in points] ,\n",
    "             '.', markersize=12)\n",
    "    plt.plot([-0.5, 2.5], [i, i], '-')\n",
    "    \n",
    "    errorSum = 0\n",
    "    errorSquareSum = 0\n",
    "    \n",
    "    for x, y in points:\n",
    "        plt.plot([x, x], [y, i], '--', color='red')\n",
    "        error = math.fabs(y-i)\n",
    "        plt.text(x + 0.02, (y + i) / 2 - 0.1, 'error=%.1f' % error, size='x-large')\n",
    "        plt.text(x + 0.02, (y + i) / 2 + 0.1, 'error^2=%.1f' % (error * error), size='x-large')\n",
    "        errorSum += error\n",
    "        errorSquareSum += error * error\n",
    "    \n",
    "    plt.text(-0.49, -0.45, 'sum of error = %.2f, sum of error^2 = %.2f' % (errorSum, errorSquareSum), size='x-large')\n",
    "    \n",
    "    plt.xlim(-0.5, 2.5)\n",
    "    plt.ylim(-0.5, 2.5)\n",
    "\n",
    "    \n",
    "# This should be executed in an Jupyter notebook!\n",
    "widgets.interact(graph, i=(0.0, 2.0));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sum of absolute errors in this case is 4 regardless of what y-coordinate we choose for our line.\n",
    "# However, the sum of squares of errors varies between 4 and 8.\n",
    "# Intuitively, using the error squared penalizes points that are further away from the line much more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Side note #2: Anscombe's quartet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before we move on, a cautionary note! Linear regression is merely a tool that has to be used appropriately.\n",
    "# Let's take a look at a set of 4 famous graphs known as Anscombe's quartet.\n",
    "\n",
    "anscombe = [\n",
    "    [[10., 8.04], [8., 6.95], [13., 7.58], [9., 8.81], [11., 8.33], [14., 9.96],\n",
    "                  [6., 7.24], [4., 4.26], [12., 10.84], [7., 4.82], [5., 5.68]],\n",
    "    [[10., 9.14], [8., 8.14], [13., 8.74], [9., 8.77], [11., 9.26], [14., 8.10],\n",
    "                  [6., 6.13], [4., 3.10], [12., 9.13], [7., 7.26], [5., 4.74]],\n",
    "    [[10., 7.46], [8., 6.77], [13., 12.74], [9., 7.11], [11., 7.81], [14., 8.84],\n",
    "                  [6., 6.08], [4., 5.39], [12., 8.15], [7., 6.42], [5., 5.73]],\n",
    "    [[8., 6.58], [8., 5.76], [8., 7.71], [8., 8.84], [8., 8.47], [8., 7.04],\n",
    "                 [8., 5.25], [19., 12.50], [8., 5.56], [8., 7.91], [8., 6.89]]\n",
    "]\n",
    "\n",
    "for i in range(4):\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    plt.plot([x for x, y in anscombe[i]], [y for x, y in anscombe[i]], '.', markersize=12)\n",
    "    plt.xlim(2, 20)\n",
    "    plt.ylim(2, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What's so interesting about the 4 data sets?\n",
    "\n",
    "properties = {}\n",
    "properties['mean_x'] = []\n",
    "properties['mean_y'] = []\n",
    "properties['var_x'] = []\n",
    "properties['var_y'] = []\n",
    "properties['coef'] = []\n",
    "properties['intercept'] = []\n",
    "for i in range(4):\n",
    "    dataframe = pd.DataFrame(anscombe[i], columns=['x', 'y'])\n",
    "    properties['mean_x'].append(dataframe['x'].mean())\n",
    "    properties['mean_y'].append(dataframe['y'].mean())\n",
    "    properties['var_x'].append(dataframe['x'].var())\n",
    "    properties['var_y'].append(dataframe['y'].var())\n",
    "    \n",
    "    print(dataframe.corr())\n",
    "        \n",
    "    model = LinearRegression()\n",
    "    model.fit(dataframe.as_matrix(['x']), np.array(dataframe['y']))\n",
    "    properties['coef'].append(model.coef_)\n",
    "    properties['intercept'].append(model.coef_)\n",
    "    \n",
    "    plt.subplot(2, 2, i+1)\n",
    "    plt.plot([x for x, y in anscombe[i]], [y for x, y in anscombe[i]], '.', markersize=12)\n",
    "    plt.plot([2, 20], [2 * model.coef_ + model.intercept_, 20 * model.coef_ + model.intercept_])\n",
    "    plt.xlim(2, 20)\n",
    "    plt.ylim(2, 14)\n",
    "    plt.title('dataset %d' % i)\n",
    "    \n",
    "pd.DataFrame(properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visually, the data looks very different, and the data in the 2 graphs on the right are clearly not linearly related.\n",
    "# However, all of them have the same basic statistical properties (mean, variance, correlation), and linear regression\n",
    "# finds nearly identical lines for all of them."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
